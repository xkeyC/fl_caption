//! Token mappings extracted from multilingual_tokenizer.json
//! Generated by extract_tokens.ps1

use once_cell::sync::Lazy;
use std::collections::HashMap;

static TOKEN_MAP: Lazy<HashMap<&'static str, u32>> = Lazy::new(|| {
    let mut map = HashMap::new();
    map.insert("<|endoftext|>", 50257);
    map.insert("<|startoftranscript|>", 50258);
    map.insert("<|en|>", 50259);
    map.insert("<|zh|>", 50260);
    map.insert("<|de|>", 50261);
    map.insert("<|es|>", 50262);
    map.insert("<|ru|>", 50263);
    map.insert("<|ko|>", 50264);
    map.insert("<|fr|>", 50265);
    map.insert("<|ja|>", 50266);
    map.insert("<|pt|>", 50267);
    map.insert("<|tr|>", 50268);
    map.insert("<|pl|>", 50269);
    map.insert("<|ca|>", 50270);
    map.insert("<|nl|>", 50271);
    map.insert("<|ar|>", 50272);
    map.insert("<|sv|>", 50273);
    map.insert("<|it|>", 50274);
    map.insert("<|id|>", 50275);
    map.insert("<|hi|>", 50276);
    map.insert("<|fi|>", 50277);
    map.insert("<|vi|>", 50278);
    map.insert("<|he|>", 50279);
    map.insert("<|uk|>", 50280);
    map.insert("<|el|>", 50281);
    map.insert("<|ms|>", 50282);
    map.insert("<|cs|>", 50283);
    map.insert("<|ro|>", 50284);
    map.insert("<|da|>", 50285);
    map.insert("<|hu|>", 50286);
    map.insert("<|ta|>", 50287);
    map.insert("<|no|>", 50288);
    map.insert("<|th|>", 50289);
    map.insert("<|ur|>", 50290);
    map.insert("<|hr|>", 50291);
    map.insert("<|bg|>", 50292);
    map.insert("<|lt|>", 50293);
    map.insert("<|la|>", 50294);
    map.insert("<|mi|>", 50295);
    map.insert("<|ml|>", 50296);
    map.insert("<|cy|>", 50297);
    map.insert("<|sk|>", 50298);
    map.insert("<|te|>", 50299);
    map.insert("<|fa|>", 50300);
    map.insert("<|lv|>", 50301);
    map.insert("<|bn|>", 50302);
    map.insert("<|sr|>", 50303);
    map.insert("<|az|>", 50304);
    map.insert("<|sl|>", 50305);
    map.insert("<|kn|>", 50306);
    map.insert("<|et|>", 50307);
    map.insert("<|mk|>", 50308);
    map.insert("<|br|>", 50309);
    map.insert("<|eu|>", 50310);
    map.insert("<|is|>", 50311);
    map.insert("<|hy|>", 50312);
    map.insert("<|ne|>", 50313);
    map.insert("<|mn|>", 50314);
    map.insert("<|bs|>", 50315);
    map.insert("<|kk|>", 50316);
    map.insert("<|sq|>", 50317);
    map.insert("<|sw|>", 50318);
    map.insert("<|gl|>", 50319);
    map.insert("<|mr|>", 50320);
    map.insert("<|pa|>", 50321);
    map.insert("<|si|>", 50322);
    map.insert("<|km|>", 50323);
    map.insert("<|sn|>", 50324);
    map.insert("<|yo|>", 50325);
    map.insert("<|so|>", 50326);
    map.insert("<|af|>", 50327);
    map.insert("<|oc|>", 50328);
    map.insert("<|ka|>", 50329);
    map.insert("<|be|>", 50330);
    map.insert("<|tg|>", 50331);
    map.insert("<|sd|>", 50332);
    map.insert("<|gu|>", 50333);
    map.insert("<|am|>", 50334);
    map.insert("<|yi|>", 50335);
    map.insert("<|lo|>", 50336);
    map.insert("<|uz|>", 50337);
    map.insert("<|fo|>", 50338);
    map.insert("<|ht|>", 50339);
    map.insert("<|ps|>", 50340);
    map.insert("<|tk|>", 50341);
    map.insert("<|nn|>", 50342);
    map.insert("<|mt|>", 50343);
    map.insert("<|sa|>", 50344);
    map.insert("<|lb|>", 50345);
    map.insert("<|my|>", 50346);
    map.insert("<|bo|>", 50347);
    map.insert("<|tl|>", 50348);
    map.insert("<|mg|>", 50349);
    map.insert("<|as|>", 50350);
    map.insert("<|tt|>", 50351);
    map.insert("<|haw|>", 50352);
    map.insert("<|ln|>", 50353);
    map.insert("<|ha|>", 50354);
    map.insert("<|ba|>", 50355);
    map.insert("<|jw|>", 50356);
    map.insert("<|su|>", 50357);
    map.insert("<|yue|>", 50358);
    map.insert("<|translate|>", 50359);
    map.insert("<|transcribe|>", 50360);
    map.insert("<|startoflm|>", 50361);
    map.insert("<|startofprev|>", 50362);
    map.insert("<|nospeech|>", 50363);
    map.insert("<|notimestamps|>", 50364);
    map
});

pub fn get_token_id(content: &str) -> Option<u32> {
    TOKEN_MAP.get(content).copied()
}

pub fn get_language_token_id(lang_code: &str) -> Option<u32> {
    let token_format = format!("<|{}|>", lang_code);
    get_token_id(&token_format)
}

pub fn is_special_token(content: &str) -> bool {
    TOKEN_MAP.contains_key(content)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_token_mappings() {

        assert_eq!(get_token_id("<|endoftext|>"), Some(50257));
        assert_eq!(get_token_id("<|startoftranscript|>"), Some(50258));
        assert_eq!(get_language_token_id("en"), Some(50259));
        assert_eq!(get_language_token_id("zh"), Some(50260));
        assert!(is_special_token("<|endoftext|>"));
        assert!(!is_special_token("not_a_token"));
    }
}
