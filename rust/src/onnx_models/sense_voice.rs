use kaldi_fbank_rust::{FbankOptions, OnlineFbank};
use ndarray::{Array1, Array2, Axis};
use ort::session::Session;
use std::collections::HashMap;
use std::time::Duration;

use crate::whisper_caption::{
    whisper::{DecodingResult, Segment, WhisperStatus},
    LaunchCaptionParams,
};

// SenseVoice ç‰¹æ®Šæ ‡è®°æšä¸¾
#[derive(Debug, Clone, PartialEq)]
pub enum SenseVoiceLanguage {
    Chinese,    // <|zh|>
    English,    // <|en|>
    Cantonese,  // <|yue|>
    Japanese,   // <|ja|>
    Korean,     // <|ko|>
    NoSpeech,   // <|nospeech|>
}

#[derive(Debug, Clone, PartialEq)]
pub enum SenseVoiceEmotion {
    Happy,      // <|HAPPY|>
    Sad,        // <|SAD|>
    Angry,      // <|ANGRY|>
    Neutral,    // <|NEUTRAL|>
    Fearful,    // <|FEARFUL|>
    Disgusted,  // <|DISGUSTED|>
    Surprised,  // <|SURPRISED|>
    Unknown,    // <|EMO_UNKNOWN|>
}

#[derive(Debug, Clone, PartialEq)]
pub enum SenseVoiceEvent {
    Speech,         // <|Speech|>
    BGM,           // <|BGM|>
    Applause,      // <|Applause|>
    Laughter,      // <|Laughter|>
    Cry,           // <|Cry|>
    Sneeze,        // <|Sneeze|>
    Breath,        // <|Breath|>
    Cough,         // <|Cough|>
    Sing,          // <|Sing|>
    SpeechNoise,   // <|Speech_Noise|>
    GBG,           // <|GBG|>
    EventUnknown,  // <|Event_UNK|>
}

#[derive(Debug, Clone, PartialEq)]
pub enum SenseVoiceTextNorm {
    WithITN,    // <|withitn|>
    WithoutITN, // <|woitn|>
}

// è§£æåçš„SenseVoiceè¾“å‡ºç»“æ„
#[derive(Debug, Clone)]
pub struct SenseVoiceOutput {
    pub language: Option<SenseVoiceLanguage>,
    pub emotion: Option<SenseVoiceEmotion>,
    pub event: Option<SenseVoiceEvent>,
    pub text_norm: Option<SenseVoiceTextNorm>,
    pub text: String,
    pub emoji: String,
}

impl SenseVoiceLanguage {
    fn from_token(token: &str) -> Option<Self> {
        match token {
            "<|zh|>" => Some(Self::Chinese),
            "<|en|>" => Some(Self::English),
            "<|yue|>" => Some(Self::Cantonese),
            "<|ja|>" => Some(Self::Japanese),
            "<|ko|>" => Some(Self::Korean),
            "<|nospeech|>" => Some(Self::NoSpeech),
            _ => None,
        }
    }

    fn to_emoji(&self) -> &'static str {
        match self {
            Self::Chinese => "",
            Self::English => "",
            Self::Cantonese => "",
            Self::Japanese => "",
            Self::Korean => "",
            Self::NoSpeech => "",
        }
    }
}

impl SenseVoiceEmotion {
    fn from_token(token: &str) -> Option<Self> {
        match token {
            "<|HAPPY|>" => Some(Self::Happy),
            "<|SAD|>" => Some(Self::Sad),
            "<|ANGRY|>" => Some(Self::Angry),
            "<|NEUTRAL|>" => Some(Self::Neutral),
            "<|FEARFUL|>" => Some(Self::Fearful),
            "<|DISGUSTED|>" => Some(Self::Disgusted),
            "<|SURPRISED|>" => Some(Self::Surprised),
            "<|EMO_UNKNOWN|>" => Some(Self::Unknown),
            _ => None,
        }
    }

    fn to_emoji(&self) -> &'static str {
        match self {
            Self::Happy => "ğŸ˜Š",
            Self::Sad => "ğŸ˜”",
            Self::Angry => "ğŸ˜¡",
            Self::Neutral => "",
            Self::Fearful => "ğŸ˜°",
            Self::Disgusted => "ğŸ¤¢",
            Self::Surprised => "ğŸ˜®",
            Self::Unknown => "",
        }
    }
}

impl SenseVoiceEvent {
    fn from_token(token: &str) -> Option<Self> {
        match token {
            "<|Speech|>" => Some(Self::Speech),
            "<|BGM|>" => Some(Self::BGM),
            "<|Applause|>" => Some(Self::Applause),
            "<|Laughter|>" => Some(Self::Laughter),
            "<|Cry|>" => Some(Self::Cry),
            "<|Sneeze|>" => Some(Self::Sneeze),
            "<|Breath|>" => Some(Self::Breath),
            "<|Cough|>" => Some(Self::Cough),
            "<|Sing|>" => Some(Self::Sing),
            "<|Speech_Noise|>" => Some(Self::SpeechNoise),
            "<|GBG|>" => Some(Self::GBG),
            "<|Event_UNK|>" => Some(Self::EventUnknown),
            _ => None,
        }
    }

    fn to_emoji(&self) -> &'static str {
        match self {
            Self::Speech => "",
            Self::BGM => "ğŸ¼",
            Self::Applause => "ğŸ‘",
            Self::Laughter => "ğŸ˜€",
            Self::Cry => "ğŸ˜­",
            Self::Sneeze => "ğŸ¤§",
            Self::Breath => "",
            Self::Cough => "ğŸ˜·",
            Self::Sing => "",
            Self::SpeechNoise => "",
            Self::GBG => "",
            Self::EventUnknown => "",
        }
    }
}

impl SenseVoiceTextNorm {
    fn from_token(token: &str) -> Option<Self> {
        match token {
            "<|withitn|>" => Some(Self::WithITN),
            "<|woitn|>" => Some(Self::WithoutITN),
            _ => None,
        }
    }
}

pub struct SenseVoiceModel {
    session: Session,
    window_size: i32,  // lfr_m
    window_shift: i32, // lfr_n
    lang_id: HashMap<String, i32>,
    with_itn: i32,
    without_itn: i32,
    neg_mean: Vec<f32>,
    inv_stddev: Vec<f32>,
}

impl SenseVoiceModel {
    pub fn from_session(session: Session) -> anyhow::Result<Self> {
        // å°è¯•è·å–è‡ªå®šä¹‰å…ƒæ•°æ®ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å€¼

        // ä»æ¨¡å‹å…ƒæ•°æ®ä¸­è·å–å‚æ•°
        let mut window_size = 7; // é»˜è®¤å€¼
        let mut window_shift = 6; // é»˜è®¤å€¼
        let mut with_itn = 1; // é»˜è®¤å€¼
        let without_itn = 0;
        
        // å°è¯•ä»metadataè·å–å®é™…å‚æ•°
        if let Ok(metadata) = session.metadata() {
            if let Ok(Some(lfr_window_size)) = metadata.custom("lfr_window_size") {
                if let Ok(size) = lfr_window_size.parse::<i32>() {
                    window_size = size;
                }
            }
            if let Ok(Some(lfr_window_shift)) = metadata.custom("lfr_window_shift") {
                if let Ok(shift) = lfr_window_shift.parse::<i32>() {
                    window_shift = shift;
                }
            }
            if let Ok(Some(with_itn_str)) = metadata.custom("with_itn") {
                if let Ok(itn) = with_itn_str.parse::<i32>() {
                    with_itn = itn;
                }
            }
        }

        // è¯­è¨€IDæ˜ å°„
        let mut lang_id = HashMap::new();
        lang_id.insert("zh".to_string(), 0);
        lang_id.insert("en".to_string(), 1);
        lang_id.insert("ja".to_string(), 2);
        lang_id.insert("ko".to_string(), 3);
        lang_id.insert("auto".to_string(), 4);

        // å½’ä¸€åŒ–å‚æ•° - æ ¹æ®window_sizeåŠ¨æ€è®¡ç®—ç»´åº¦
        let feature_dim = 80 * window_size as usize;
        let mut neg_mean = vec![0.0; feature_dim];
        let mut inv_stddev = vec![1.0; feature_dim];
        
        // ä»metadataè·å–å½’ä¸€åŒ–å‚æ•°
        if let Ok(metadata) = session.metadata() {
            if let Ok(Some(inv_stddev_str)) = metadata.custom("inv_stddev") {
                let values: Vec<f32> = inv_stddev_str
                    .split(',')
                    .filter_map(|s| s.trim().parse().ok())
                    .collect();
                if values.len() == feature_dim {
                    inv_stddev = values;
                }
            }
            if let Ok(Some(neg_mean_str)) = metadata.custom("neg_mean") {
                let values: Vec<f32> = neg_mean_str
                    .split(',')
                    .filter_map(|s| s.trim().parse().ok())
                    .collect();
                if values.len() == feature_dim {
                    neg_mean = values;
                }
            }
        }

        println!("SenseVoice model parameters loaded:");
        println!("  - window_size (lfr_m): {}", window_size);
        println!("  - window_shift (lfr_n): {}", window_shift);
        println!("  - with_itn: {}, without_itn: {}", with_itn, without_itn);
        println!(
            "  - neg_mean length: {}, inv_stddev length: {}",
            neg_mean.len(),
            inv_stddev.len()
        );
        println!("  - language mappings: {:?}", lang_id);

        // å°è¯•æ‰“å°å¯ç”¨çš„å…ƒæ•°æ®ä¿¡æ¯ç”¨äºè°ƒè¯•
        if let Ok(metadata) = session.metadata() {
            println!("Available metadata:");
            println!("  - version: {:?}", metadata.version());
            println!("  - with_itn: {:?}", metadata.custom("with_itn"));
            println!("  - lfr_window_size: {:?}", metadata.custom("lfr_window_size"));
            println!("  - lfr_window_shift: {:?}", metadata.custom("lfr_window_shift"));
        }

        // æ‰“å°æ¨¡å‹çš„è¾“å…¥ä¿¡æ¯ç”¨äºè°ƒè¯•
        println!("Model inputs:");
        for input in session.inputs.iter() {
            println!("  - name: {}, type: {:?}", input.name, input.input_type);
        }

        // æ‰“å°æ¨¡å‹çš„è¾“å‡ºä¿¡æ¯ç”¨äºè°ƒè¯•
        println!("Model outputs:");
        for output in session.outputs.iter() {
            println!("  - name: {}, type: {:?}", output.name, output.output_type);
        }

        Ok(Self {
            session,
            window_size,
            window_shift,
            lang_id,
            with_itn,
            without_itn,
            neg_mean,
            inv_stddev,
        })
    }

    #[allow(dead_code)]
    pub fn inference(
        &mut self,
        features: Array2<f32>,
        language: &str,
        use_itn: bool,
    ) -> anyhow::Result<Array2<f32>> {
        use ort::value::Value;

        let seq_len = features.shape()[0];

        // å‡†å¤‡è¾“å…¥å¼ é‡
        let x = features.insert_axis(Axis(0)); // [1, T, D] - æ·»åŠ batchç»´åº¦

        // æ ¹æ®æ¨¡å‹è¾“å…¥è¦æ±‚ï¼Œx_length, language, text_norm éœ€è¦æ˜¯ i32 ç±»å‹
        let x_length = Array1::from_vec(vec![seq_len as i32]);

        let language_id = self
            .lang_id
            .get(language)
            .copied()
            .unwrap_or(self.lang_id.get("auto").copied().unwrap_or(4));
        let language_tensor = Array1::from_vec(vec![language_id]);

        let text_norm_id = if use_itn {
            self.with_itn
        } else {
            self.without_itn
        };
        let text_norm_tensor = Array1::from_vec(vec![text_norm_id]);

        // to value
        let x_value = Value::from_array(x)?;
        let x_length_value = Value::from_array(x_length)?;
        let language_value = Value::from_array(language_tensor)?;
        let text_norm_value = Value::from_array(text_norm_tensor)?;

        let outputs = self.session.run(ort::inputs![
            "x" => x_value,
            "x_length" => x_length_value,
            "language" => language_value,
            "text_norm" => text_norm_value,
        ])?;

        // è·å–è¾“å‡º
        let output_keys: Vec<_> = outputs.keys().collect();
        if output_keys.is_empty() {
            return Err(anyhow::anyhow!("No outputs from model"));
        }

        println!("Available output keys: {:?}", output_keys);

        // å°è¯•è·å–è¾“å‡º - å¸¸è§çš„è¾“å‡ºåç§°
        let logits_value = outputs
            .get("logits")
            .or_else(|| outputs.get("output"))
            .or_else(|| outputs.get("outputs"))
            .or_else(|| outputs.get(output_keys[0]))
            .ok_or_else(|| anyhow::anyhow!("Cannot find model output"))?;

        // å°†ONNX Valueè½¬æ¢ä¸ºndarray
        match logits_value.try_extract_tensor::<f32>() {
            Ok(tensor_data) => {
                let (shape, data) = tensor_data;

                // å‡è®¾è¾“å‡ºå½¢çŠ¶ä¸º [batch, seq_len, vocab_size] æˆ– [seq_len, vocab_size]
                if shape.len() < 2 {
                    return Err(anyhow::anyhow!(
                        "Expected at least 2D output tensor, got {}D",
                        shape.len()
                    ));
                }

                // å¤„ç†ä¸åŒçš„è¾“å‡ºå½¢çŠ¶ï¼Œè½¬æ¢ä¸ºusize
                let (seq_len_out, vocab_size) = if shape.len() == 3 {
                    // å½¢çŠ¶ä¸º [batch, seq_len, vocab_size]
                    let batch_size = shape[0] as usize;
                    if batch_size != 1 {
                        return Err(anyhow::anyhow!("Expected batch size 1, got {}", batch_size));
                    }
                    (shape[1] as usize, shape[2] as usize)
                } else {
                    // å½¢çŠ¶ä¸º [seq_len, vocab_size]
                    (shape[0] as usize, shape[1] as usize)
                };

                // åˆ›å»º2Dæ•°ç»„ [seq_len, vocab_size]
                let mut logits = Array2::zeros((seq_len_out, vocab_size));

                // è®¡ç®—æ•°æ®åç§»é‡
                let data_offset = if shape.len() == 3 { 0 } else { 0 };

                for i in 0..seq_len_out {
                    for j in 0..vocab_size {
                        let idx = data_offset + i * vocab_size + j;
                        if idx < data.len() {
                            logits[[i, j]] = data[idx];
                        }
                    }
                }

                Ok(logits)
            }
            Err(e) => Err(anyhow::anyhow!("Failed to extract tensor: {}", e)),
        }
    }
}

#[allow(dead_code)]
fn load_tokens(tokens_path: &str) -> anyhow::Result<HashMap<usize, String>> {
    let content = std::fs::read_to_string(tokens_path)?;
    let mut tokens = HashMap::new();

    for (i, line) in content.lines().enumerate() {
        if let Some(token) = line.split_whitespace().next() {
            tokens.insert(i, token.to_string());
        }
    }

    Ok(tokens)
}

fn compute_features(
    samples: &[f32],
    sample_rate: f32,
    neg_mean: &[f32],
    inv_stddev: &[f32],
    window_size: i32,
    window_shift: i32,
) -> anyhow::Result<Array2<f32>> {
    // é…ç½®fbankå‚æ•°
    let mut fbank_opts = FbankOptions::default();
    fbank_opts.frame_opts.dither = 0.0;
    fbank_opts.frame_opts.snip_edges = false;
    fbank_opts.frame_opts.samp_freq = sample_rate;
    // è®¾ç½®hammingçª—å£
    fbank_opts.frame_opts.window_type = std::ffi::CStr::from_bytes_with_nul(b"hamming\0")
        .unwrap()
        .as_ptr(); // "hamming";
    fbank_opts.mel_opts.num_bins = 80;

    let mut online_fbank = OnlineFbank::new(fbank_opts);

    // å°†æ ·æœ¬ç¼©æ”¾åˆ°16ä½æ•´æ•°èŒƒå›´ç„¶åè½¬æ¢ä¸ºf32
    let scaled_samples: Vec<f32> = samples.iter().map(|&x| x * 32768.0).collect();
    online_fbank.accept_waveform(sample_rate, &scaled_samples);
    online_fbank.input_finished();

    let num_frames = online_fbank.num_ready_frames();
    if num_frames == 0 {
        return Err(anyhow::anyhow!("No frames ready"));
    }

    // æå–æ‰€æœ‰å¸§çš„ç‰¹å¾
    let mut features = Vec::with_capacity(num_frames as usize);
    for i in 0..num_frames {
        if let Some(frame) = online_fbank.get_frame(i) {
            features.push(frame.to_vec());
        }
    }

    if features.is_empty() {
        return Err(anyhow::anyhow!("No features extracted"));
    }

    let feature_dim = features[0].len();
    let mut feature_matrix = Array2::zeros((features.len(), feature_dim));

    for (i, frame) in features.iter().enumerate() {
        for (j, &val) in frame.iter().enumerate() {
            feature_matrix[[i, j]] = val;
        }
    }

    // åº”ç”¨LFR (Low Frame Rate) å¤„ç†
    let lfr_features = apply_lfr(&feature_matrix, window_size, window_shift)?;

    // åº”ç”¨å½’ä¸€åŒ–
    let normalized_features = apply_normalization(&lfr_features, neg_mean, inv_stddev)?;

    Ok(normalized_features)
}

#[allow(dead_code)]
fn apply_lfr(
    features: &Array2<f32>,
    window_size: i32,
    window_shift: i32,
) -> anyhow::Result<Array2<f32>> {
    let (num_frames, feature_dim) = features.dim();

    if num_frames < window_size as usize {
        return Err(anyhow::anyhow!("Not enough frames for LFR processing"));
    }

    let t = (num_frames - window_size as usize) / window_shift as usize + 1;
    let output_dim = feature_dim * window_size as usize;

    let mut lfr_features = Array2::zeros((t, output_dim));

    for i in 0..t {
        let start_frame = i * window_shift as usize;
        for j in 0..window_size as usize {
            let frame_idx = start_frame + j;
            let output_start = j * feature_dim;
            for k in 0..feature_dim {
                lfr_features[[i, output_start + k]] = features[[frame_idx, k]];
            }
        }
    }

    Ok(lfr_features)
}

fn apply_normalization(
    features: &Array2<f32>,
    neg_mean: &[f32],
    inv_stddev: &[f32],
) -> anyhow::Result<Array2<f32>> {
    let mut normalized = features.clone();

    for mut row in normalized.rows_mut() {
        for (i, val) in row.iter_mut().enumerate() {
            if i < neg_mean.len() && i < inv_stddev.len() {
                *val = (*val + neg_mean[i]) * inv_stddev[i];
            }
        }
    }

    Ok(normalized)
}

fn parse_sensevoice_output(logits: &Array2<f32>, tokens: &HashMap<usize, String>) -> SenseVoiceOutput {
    // è·å–æœ€å¤§æ¦‚ç‡çš„tokenç´¢å¼•
    let indices: Vec<usize> = logits
        .rows()
        .into_iter()
        .map(|row| {
            row.iter()
                .enumerate()
                .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
                .map(|(idx, _)| idx)
                .unwrap_or(0)
        })
        .collect();

    // å»é™¤è¿ç»­é‡å¤çš„token
    let mut unique_indices = Vec::new();
    let mut prev_idx = None;
    for idx in indices {
        if prev_idx != Some(idx) {
            unique_indices.push(idx);
            prev_idx = Some(idx);
        }
    }

    // å»é™¤blank token (é€šå¸¸æ˜¯0)
    let blank_id = 0;
    unique_indices.retain(|&idx| idx != blank_id);

    // è½¬æ¢ä¸ºtokenå­—ç¬¦ä¸²
    let token_strings: Vec<String> = unique_indices
        .iter()
        .filter_map(|&idx| tokens.get(&idx))
        .map(|token| token.to_string())
        .collect();

    // å°†æ‰€æœ‰tokenè¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²è¿›è¡Œåˆæ­¥å¤„ç†
    let full_text = token_strings.join("");
    
    // è§£æç‰¹æ®Šæ ‡è®°
    let mut language = None;
    let mut emotion = None;
    let mut event = None;
    let mut text_norm = None;
    let mut text_parts = Vec::new();
    let mut emoji_parts = Vec::new();

    // å¤„ç†ç‰¹æ®Šçš„ç»„åˆæ ‡è®°
    let processed_text = full_text
        .replace("<|nospeech|><|Event_UNK|>", "â“"); // ç‰¹æ®Šç»„åˆæ ‡è®°
    
    // ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ–ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…æ¥æå–ç‰¹æ®Šæ ‡è®°å’Œæ™®é€šæ–‡æœ¬
    let mut remaining_text = processed_text.as_str();
    
    // æŒ‰é¡ºåºå¤„ç†å„ç§æ ‡è®°
    while let Some(start) = remaining_text.find("<|") {
        // æ·»åŠ æ ‡è®°å‰çš„æ–‡æœ¬
        if start > 0 {
            let before_text = &remaining_text[..start];
            if !before_text.trim().is_empty() {
                text_parts.push(before_text.to_string());
            }
        }
        
        // æŸ¥æ‰¾æ ‡è®°ç»“æŸ
        if let Some(end) = remaining_text[start..].find("|>") {
            let tag_end = start + end + 2;
            let tag = &remaining_text[start..tag_end];
            
            // è§£ææ ‡è®°
            if let Some(lang) = SenseVoiceLanguage::from_token(tag) {
                language = Some(lang.clone());
                let emoji = lang.to_emoji();
                if !emoji.is_empty() {
                    emoji_parts.push(emoji.to_string());
                }
            } else if let Some(emo) = SenseVoiceEmotion::from_token(tag) {
                emotion = Some(emo.clone());
                let emoji = emo.to_emoji();
                if !emoji.is_empty() {
                    emoji_parts.push(emoji.to_string());
                }
            } else if let Some(evt) = SenseVoiceEvent::from_token(tag) {
                event = Some(evt.clone());
                let emoji = evt.to_emoji();
                if !emoji.is_empty() {
                    emoji_parts.push(emoji.to_string());
                }
            } else if let Some(norm) = SenseVoiceTextNorm::from_token(tag) {
                text_norm = Some(norm);
            }
            // å…¶ä»–æœªè¯†åˆ«çš„æ ‡è®°è¢«å¿½ç•¥
            
            remaining_text = &remaining_text[tag_end..];
        } else {
            // å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æŸæ ‡è®°ï¼Œå°†å‰©ä½™éƒ¨åˆ†ä½œä¸ºæ–‡æœ¬
            text_parts.push(remaining_text.to_string());
            break;
        }
    }
    
    // æ·»åŠ å‰©ä½™çš„æ–‡æœ¬
    if !remaining_text.is_empty() && !remaining_text.trim().is_empty() {
        text_parts.push(remaining_text.to_string());
    }

    // åˆå¹¶æ–‡æœ¬å¹¶æ¸…ç†
    let text = text_parts
        .join("")
        .replace("â–", " ")
        .trim()
        .to_string();

    let emoji = emoji_parts.join("");

    SenseVoiceOutput {
        language,
        emotion,
        event,
        text_norm,
        text,
        emoji,
    }
}

pub async fn launch_caption<F>(
    session: Session,
    params: LaunchCaptionParams,
    mut result_callback: F,
) -> anyhow::Result<()>
where
    F: FnMut(Vec<Segment>) + Send + 'static,
{
    use crate::audio_capture::{AudioCapture, AudioCaptureConfig, PlatformAudioCapture};
    use crate::vad;
    use std::time::Duration;
    use tokio::time::Instant;

    let LaunchCaptionParams {
        audio_device,
        audio_device_is_input,
        audio_language,
        cancel_token,
        tokenizer_data,
        inference_timeout,
        inference_interval_ms,
        vad_model_path,
        vad_filters_value,
        whisper_max_audio_duration,
        ..
    } = params;

    // åˆå§‹åŒ–SenseVoiceæ¨¡å‹
    result_callback(_make_status_response(WhisperStatus::Loading));
    let mut model = SenseVoiceModel::from_session(session)?;

    // åŠ è½½tokensæ˜ å°„
    let tokenizer_str = std::str::from_utf8(&tokenizer_data)?;
    let tokens = load_tokens_from_data(tokenizer_str)?;

    // è®¾ç½®éŸ³é¢‘æ•è·é…ç½®
    let audio_capture_config = AudioCaptureConfig {
        device: audio_device,
        is_input: audio_device_is_input.unwrap_or(true),
        target_sample_rate: 16000,
        target_channels: 1,
    };

    let audio_capture = PlatformAudioCapture::new(audio_capture_config)?;
    let audio_info = audio_capture.get_info();
    println!("SenseVoice Audio capture info: {:?}", audio_info);

    // å¼€å§‹éŸ³é¢‘æ•è·
    let rx = audio_capture.start_capture(cancel_token.child_token())?;

    result_callback(_make_status_response(WhisperStatus::Ready));
    println!("SenseVoice Ready...");

    // åˆå§‹åŒ–éŸ³é¢‘å¤„ç†çŠ¶æ€
    let mut buffered_pcm = vec![];
    let mut history_pcm = Vec::new();
    let mut last_inference_time = Instant::now();
    let mut first_inference_done = false;
    let inference_interval = Duration::from_millis(inference_interval_ms.unwrap_or(2000)); // é»˜è®¤2000æ¯«ç§’
    let max_audio_duration: usize = whisper_max_audio_duration.unwrap_or(12) as usize; // é»˜è®¤12ç§’
    let language = audio_language.as_deref().unwrap_or("auto"); // SenseVoiceè¯­è¨€è®¾ç½®

    println!("Check and loading VAD model...");
    let vad_model = if let Some(vad_model_path) = vad_model_path {
        let model = vad::new_vad_model(vad_model_path, false);
        if let Ok(model) = model {
            Some(model)
        } else {
            println!("Failed to load VAD model: {:?}", model.err().unwrap());
            None
        }
    } else {
        None
    };

    // éŸ³é¢‘å¤„ç†ä¸»å¾ªç¯
    println!("Starting SenseVoice audio processing loop...");
    let mut debug_counter = 0;

    while !cancel_token.is_cancelled() {
        debug_counter += 1;
        if debug_counter % 500 == 0 {
            println!(
                "SenseVoice audio processing loop iteration {}, buffered_pcm.len(): {}",
                debug_counter,
                buffered_pcm.len()
            );
        }

        // æ¥æ”¶éŸ³é¢‘æ•°æ®
        let pcm = rx.recv_timeout(Duration::from_millis(100));

        if pcm.is_err() {
            let err = pcm.unwrap_err();
            if debug_counter % 1000 == 0 {
                println!(
                    "Audio recv timeout or error: {:?}, cancel_token cancelled: {}",
                    err,
                    cancel_token.is_cancelled()
                );
            }
            if cancel_token.is_cancelled() {
                break;
            }
            continue;
        }

        let pcm = pcm.unwrap();

        static mut AUDIO_RECEIVED: bool = false;
        unsafe {
            if !AUDIO_RECEIVED {
                println!(
                    "SenseVoice first audio data received: {} samples",
                    pcm.len()
                );
                AUDIO_RECEIVED = true;
            } else if pcm.len() > 0 && debug_counter % 100 == 0 {
                println!(
                    "SenseVoice audio data: {} samples (debug every 100 iterations)",
                    pcm.len()
                );
            }
        }

        buffered_pcm.extend_from_slice(&pcm);

        if buffered_pcm.len() > 0 && (buffered_pcm.len() % 16000 == 0 || debug_counter % 200 == 0) {
            println!(
                "Total buffered_pcm length: {} samples ({:.1}s)",
                buffered_pcm.len(),
                buffered_pcm.len() as f32 / 16000.0
            );
        }

        // é¦–æ¬¡å¯åŠ¨æ—¶ï¼Œç­‰å¾…3ç§’æ•°æ®
        if !first_inference_done {
            if buffered_pcm.len() < 3 * 16000 {
                continue;
            }
            first_inference_done = true;
        }

        // æ£€æŸ¥æ¨ç†é—´éš”
        let now = Instant::now();
        if now.duration_since(last_inference_time) < inference_interval {
            continue;
        }

        // è®°å½•æ¨ç†å¼€å§‹æ—¶é—´
        let inference_start = Instant::now();

        // VADæ£€æµ‹
        if let Some(vad_model) = &vad_model {
            let resampled_pcm = buffered_pcm.clone();
            let vad_result = vad_model.check_vad(resampled_pcm, vad_filters_value);
            if vad_result.is_err() {
                println!("VAD error: {:?}", vad_result.err().unwrap());
            } else {
                let vad_result = vad_result?;
                println!(
                    "SenseVoice VAD prediction: {:?} filtered_count: {:?}",
                    vad_result.prediction, vad_result.filtered_count
                );
                if vad_result.prediction > vad_filters_value.unwrap_or(0.1) {
                    buffered_pcm = vad_result.pcm_results;
                } else {
                    buffered_pcm.clear();
                    last_inference_time = Instant::now();
                    continue;
                }
            }
        }

        // éŸ³é¢‘é•¿åº¦ç®¡ç† - ä¸Whisperç›¸åŒçš„é€»è¾‘
        let max_samples = max_audio_duration * 16000;
        let total_len = history_pcm.len() + buffered_pcm.len();

        let mut adjusted_history_pcm = history_pcm.clone();
        if total_len > max_samples {
            let excess = total_len - max_samples;
            println!(
                "SenseVoice history_pcm len: {} buffered_pcm len: {} excess: {}",
                history_pcm.len(),
                buffered_pcm.len(),
                excess
            );
            if history_pcm.len() > excess {
                adjusted_history_pcm = history_pcm[excess..].to_vec();
            } else {
                adjusted_history_pcm = Vec::new();
            }
        }

        // åˆå¹¶éŸ³é¢‘æ•°æ®
        let mut combined_pcm = Vec::with_capacity(adjusted_history_pcm.len() + buffered_pcm.len());
        combined_pcm.extend_from_slice(&adjusted_history_pcm);
        combined_pcm.extend_from_slice(&buffered_pcm);

        history_pcm = combined_pcm.clone();
        buffered_pcm.clear();

        let pcm = combined_pcm;

        // SenseVoiceç‰¹å¾æå–å’Œæ¨ç†
        match run_sensevoice_inference(
            &mut model,
            &pcm,
            language,
            &tokens,
            inference_timeout.or(Some(inference_interval)),
        ) {
            Ok(segments) => {
                let inference_duration = inference_start.elapsed();
                let audio_duration = (pcm.len() as f32 / 16000.0 * 1000.0) as u128;

                let mut result_segments = segments;
                for segment in &mut result_segments {
                    segment.reasoning_duration = Some(inference_duration.as_millis());
                    segment.reasoning_lang = Some(language.to_string());
                    segment.audio_duration = Some(audio_duration);
                }

                result_callback(result_segments);
            }
            Err(e) => {
                println!("SenseVoice inference error: {:?}", e);
                // å‘é€é”™è¯¯çŠ¶æ€
                result_callback(_make_status_response(WhisperStatus::Error));
            }
        }

        last_inference_time = now;
    }

    println!("SenseVoice transcription cancelled");
    result_callback(_make_status_response(WhisperStatus::Exit));
    println!("SenseVoice Exit");
    Ok(())
}

// SenseVoiceæ¨ç†è¾…åŠ©å‡½æ•°
fn run_sensevoice_inference(
    model: &mut SenseVoiceModel,
    pcm: &[f32],
    language: &str,
    tokens: &HashMap<usize, String>,
    #[allow(unused_variables)] timeout: Option<Duration>,
) -> anyhow::Result<Vec<Segment>> {
    // è®¡ç®—ç‰¹å¾
    let features = compute_features(
        pcm,
        16000.0,
        &model.neg_mean,
        &model.inv_stddev,
        model.window_size,
        model.window_shift,
    )?;

    // è¿è¡Œæ¨ç†
    let use_itn = false; // å¯ä»¥æ ¹æ®éœ€è¦é…ç½®
    let logits = model.inference(features, language, use_itn)?;

    // è§£æSenseVoiceè¾“å‡º
    let parsed_output = parse_sensevoice_output(&logits, tokens);
    
    // æ‰“å°è°ƒè¯•ä¿¡æ¯
    println!("SenseVoice parsed output:");
    println!("  Language: {:?}", parsed_output.language);
    println!("  Emotion: {:?}", parsed_output.emotion);
    println!("  Event: {:?}", parsed_output.event);
    println!("  Text norm: {:?}", parsed_output.text_norm);
    println!("  Emoji: {}", parsed_output.emoji);
    println!("  Clean text: {}", parsed_output.text);

    // åˆ›å»ºsegmentï¼Œåªè¿”å›çº¯æ–‡æœ¬
    let duration = pcm.len() as f64 / 16000.0;
    let segment = Segment {
        start: 0.0,
        duration,
        dr: DecodingResult {
            tokens: vec![], // SenseVoiceæš‚ä¸è¿”å›tokenåºåˆ—
            text: parsed_output.text, // åªè¿”å›çº¯æ–‡æœ¬ï¼Œä¸åŒ…å«ç‰¹æ®Šæ ‡è®°
            avg_logprob: 0.0,
            no_speech_prob: 0.0,
            temperature: 0.0,
            compression_ratio: 1.0,
        },
        reasoning_duration: None,
        reasoning_lang: None,
        audio_duration: None,
        status: WhisperStatus::Working,
    };

    Ok(vec![segment])
}

fn _make_status_response(status: WhisperStatus) -> Vec<Segment> {
    vec![Segment {
        start: 0.0,
        duration: 0.0,
        dr: DecodingResult {
            tokens: vec![],
            text: "".to_string(),
            avg_logprob: 0.0,
            no_speech_prob: 0.0,
            temperature: 0.0,
            compression_ratio: 0.0,
        },
        reasoning_duration: None,
        reasoning_lang: None,
        audio_duration: None,
        status,
    }]
}

fn load_tokens_from_data(data: &str) -> anyhow::Result<HashMap<usize, String>> {
    let mut tokens = HashMap::new();

    for (i, line) in data.lines().enumerate() {
        if let Some(token) = line.split_whitespace().next() {
            tokens.insert(i, token.to_string());
        }
    }

    Ok(tokens)
}
